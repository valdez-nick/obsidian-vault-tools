"""
Real-time Monitoring System Implementation

Comprehensive monitoring solution for PM performance metrics with anomaly detection,
alerting, and automated response capabilities.
"""

import asyncio
import logging
import json
from typing import Dict, Any, List, Optional, Callable, Set
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import statistics
from collections import deque
import threading
import time

try:
    import prometheus_client
    from prometheus_client import Counter, Gauge, Histogram, Summary
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

logger = logging.getLogger(__name__)


class MetricType(Enum):
    """Types of metrics to monitor."""
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"


class AlertSeverity(Enum):
    """Alert severity levels."""
    INFO = 1
    WARNING = 2
    ERROR = 3
    CRITICAL = 4


class AnomalyDetectionMethod(Enum):
    """Methods for anomaly detection."""
    THRESHOLD = "threshold"
    STATISTICAL = "statistical"
    TREND = "trend"
    PATTERN = "pattern"
    ML_BASED = "ml_based"


@dataclass
class MetricDefinition:
    """Definition of a metric to monitor."""
    name: str
    metric_type: MetricType
    description: str
    unit: Optional[str] = None
    labels: List[str] = field(default_factory=list)
    buckets: Optional[List[float]] = None  # For histograms
    aggregation_window: int = 60  # seconds
    retention_period: int = 86400  # seconds (24 hours)


@dataclass
class AlertRule:
    """Rule for generating alerts."""
    name: str
    metric_name: str
    condition: str  # Expression like "value > 100"
    severity: AlertSeverity
    detection_method: AnomalyDetectionMethod = AnomalyDetectionMethod.THRESHOLD
    threshold_value: Optional[float] = None
    window_size: int = 300  # seconds
    cooldown_period: int = 600  # seconds
    actions: List[str] = field(default_factory=list)
    enabled: bool = True
    
    def evaluate(self, value: float, historical_values: List[float] = None) -> bool:
        """Evaluate if alert should be triggered."""
        if self.detection_method == AnomalyDetectionMethod.THRESHOLD:
            return eval(self.condition.replace('value', str(value)))
        
        elif self.detection_method == AnomalyDetectionMethod.STATISTICAL and historical_values:
            if len(historical_values) < 10:
                return False
            
            mean = statistics.mean(historical_values)
            stdev = statistics.stdev(historical_values) if len(historical_values) > 1 else 0
            z_score = (value - mean) / (stdev + 1e-6)
            
            return abs(z_score) > 3  # 3 standard deviations
        
        elif self.detection_method == AnomalyDetectionMethod.TREND and historical_values:
            if len(historical_values) < 3:
                return False
            
            # Simple trend detection
            recent_avg = statistics.mean(historical_values[-3:])
            older_avg = statistics.mean(historical_values[:-3])
            trend_change = abs(recent_avg - older_avg) / (older_avg + 1e-6)
            
            return trend_change > 0.5  # 50% change
        
        return False


@dataclass
class PerformanceThreshold:
    """Performance thresholds for different metrics."""
    metric_name: str
    warning_threshold: float
    critical_threshold: float
    direction: str = "above"  # "above" or "below"
    
    def check_threshold(self, value: float) -> Optional[AlertSeverity]:
        """Check if value crosses thresholds."""
        if self.direction == "above":
            if value >= self.critical_threshold:
                return AlertSeverity.CRITICAL
            elif value >= self.warning_threshold:
                return AlertSeverity.WARNING
        else:  # below
            if value <= self.critical_threshold:
                return AlertSeverity.CRITICAL
            elif value <= self.warning_threshold:
                return AlertSeverity.WARNING
        
        return None


@dataclass
class MonitoringEvent:
    """Event generated by the monitoring system."""
    timestamp: datetime
    event_type: str  # "metric_update", "alert_triggered", "anomaly_detected"
    metric_name: str
    value: Any
    metadata: Dict[str, Any] = field(default_factory=dict)
    severity: Optional[AlertSeverity] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            'timestamp': self.timestamp.isoformat(),
            'event_type': self.event_type,
            'metric_name': self.metric_name,
            'value': self.value,
            'metadata': self.metadata,
            'severity': self.severity.name if self.severity else None
        }


class MetricStore:
    """In-memory storage for metrics with time-based retention."""
    
    def __init__(self, retention_period: int = 86400):
        """
        Initialize metric store.
        
        Args:
            retention_period: How long to keep metrics in seconds
        """
        self.retention_period = retention_period
        self.metrics: Dict[str, deque] = {}
        self._lock = threading.Lock()
    
    def add_metric(self, name: str, value: float, timestamp: Optional[datetime] = None):
        """Add a metric value."""
        if timestamp is None:
            timestamp = datetime.now()
        
        with self._lock:
            if name not in self.metrics:
                self.metrics[name] = deque()
            
            self.metrics[name].append((timestamp, value))
            
            # Clean old values
            self._cleanup_metric(name)
    
    def get_metric_values(self, name: str, window_seconds: Optional[int] = None) -> List[float]:
        """Get metric values within time window."""
        with self._lock:
            if name not in self.metrics:
                return []
            
            if window_seconds is None:
                return [value for _, value in self.metrics[name]]
            
            cutoff_time = datetime.now() - timedelta(seconds=window_seconds)
            return [value for timestamp, value in self.metrics[name] if timestamp > cutoff_time]
    
    def get_latest_value(self, name: str) -> Optional[float]:
        """Get most recent metric value."""
        with self._lock:
            if name not in self.metrics or not self.metrics[name]:
                return None
            
            return self.metrics[name][-1][1]
    
    def _cleanup_metric(self, name: str):
        """Remove old metric values."""
        cutoff_time = datetime.now() - timedelta(seconds=self.retention_period)
        
        while self.metrics[name] and self.metrics[name][0][0] < cutoff_time:
            self.metrics[name].popleft()


class MonitoringSystem:
    """
    Comprehensive monitoring system for PM performance metrics.
    
    Features real-time metric collection, anomaly detection, alerting,
    and integration with monitoring platforms.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize Monitoring System.
        
        Args:
            config: System configuration
        """
        self.config = config
        self.metric_definitions: Dict[str, MetricDefinition] = {}
        self.alert_rules: Dict[str, AlertRule] = {}
        self.performance_thresholds: Dict[str, PerformanceThreshold] = {}
        
        # Metric storage
        self.metric_store = MetricStore(
            retention_period=config.get('retention_period', 86400)
        )
        
        # Alert management
        self.alert_history: List[MonitoringEvent] = []
        self.alert_cooldowns: Dict[str, datetime] = {}
        
        # Event handling
        self.event_handlers: List[Callable] = []
        self.running = False
        self._monitor_task = None
        
        # Prometheus metrics if available
        self.prometheus_metrics = {}
        if PROMETHEUS_AVAILABLE and config.get('enable_prometheus', True):
            self._initialize_prometheus_metrics()
        
        # System metrics collection
        self.collect_system_metrics = config.get('collect_system_metrics', True)
        
    def _initialize_prometheus_metrics(self):
        """Initialize Prometheus metrics."""
        # Standard PM metrics
        self.prometheus_metrics['pm_velocity'] = Gauge(
            'pm_velocity_points',
            'Team velocity in story points',
            ['team', 'sprint']
        )
        
        self.prometheus_metrics['pm_burnout_risk'] = Gauge(
            'pm_burnout_risk_score',
            'PM burnout risk score',
            ['pm_name']
        )
        
        self.prometheus_metrics['pm_quality_score'] = Gauge(
            'pm_quality_score',
            'Code quality score',
            ['team', 'project']
        )
        
        self.prometheus_metrics['pm_tasks_completed'] = Counter(
            'pm_tasks_completed_total',
            'Total tasks completed',
            ['pm_name', 'task_type']
        )
        
        self.prometheus_metrics['pm_response_time'] = Histogram(
            'pm_response_time_seconds',
            'PM response time to requests',
            ['request_type'],
            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
        )
    
    def define_metric(self, metric_def: MetricDefinition):
        """
        Define a new metric to monitor.
        
        Args:
            metric_def: Metric definition
        """
        self.metric_definitions[metric_def.name] = metric_def
        
        # Create Prometheus metric if enabled
        if PROMETHEUS_AVAILABLE and metric_def.name not in self.prometheus_metrics:
            if metric_def.metric_type == MetricType.COUNTER:
                self.prometheus_metrics[metric_def.name] = Counter(
                    metric_def.name,
                    metric_def.description,
                    metric_def.labels
                )
            elif metric_def.metric_type == MetricType.GAUGE:
                self.prometheus_metrics[metric_def.name] = Gauge(
                    metric_def.name,
                    metric_def.description,
                    metric_def.labels
                )
            elif metric_def.metric_type == MetricType.HISTOGRAM:
                self.prometheus_metrics[metric_def.name] = Histogram(
                    metric_def.name,
                    metric_def.description,
                    metric_def.labels,
                    buckets=metric_def.buckets or prometheus_client.DEFAULT_BUCKETS
                )
        
        logger.info(f"Defined metric: {metric_def.name}")
    
    def add_alert_rule(self, rule: AlertRule):
        """
        Add an alert rule.
        
        Args:
            rule: Alert rule configuration
        """
        self.alert_rules[rule.name] = rule
        logger.info(f"Added alert rule: {rule.name}")
    
    def set_performance_threshold(self, threshold: PerformanceThreshold):
        """
        Set performance threshold for a metric.
        
        Args:
            threshold: Performance threshold configuration
        """
        self.performance_thresholds[threshold.metric_name] = threshold
        logger.info(f"Set performance threshold for: {threshold.metric_name}")
    
    def record_metric(self, name: str, value: float, labels: Optional[Dict[str, str]] = None):
        """
        Record a metric value.
        
        Args:
            name: Metric name
            value: Metric value
            labels: Optional labels for the metric
        """
        # Store in internal storage
        self.metric_store.add_metric(name, value)
        
        # Update Prometheus metric if available
        if name in self.prometheus_metrics:
            metric = self.prometheus_metrics[name]
            
            if labels and hasattr(metric, 'labels'):
                metric.labels(**labels).set(value)
            elif hasattr(metric, 'set'):
                metric.set(value)
            elif hasattr(metric, 'inc'):
                metric.inc(value)
        
        # Check thresholds
        if name in self.performance_thresholds:
            threshold = self.performance_thresholds[name]
            severity = threshold.check_threshold(value)
            
            if severity:
                self._trigger_threshold_alert(name, value, severity, threshold)
        
        # Emit metric update event
        event = MonitoringEvent(
            timestamp=datetime.now(),
            event_type="metric_update",
            metric_name=name,
            value=value,
            metadata={'labels': labels} if labels else {}
        )
        
        self._emit_event(event)
    
    def _trigger_threshold_alert(self, metric_name: str, value: float, 
                                severity: AlertSeverity, threshold: PerformanceThreshold):
        """Trigger alert for threshold violation."""
        alert_name = f"{metric_name}_threshold_{severity.name.lower()}"
        
        # Check cooldown
        if alert_name in self.alert_cooldowns:
            if datetime.now() < self.alert_cooldowns[alert_name]:
                return
        
        # Create alert event
        event = MonitoringEvent(
            timestamp=datetime.now(),
            event_type="alert_triggered",
            metric_name=metric_name,
            value=value,
            severity=severity,
            metadata={
                'threshold_type': threshold.direction,
                'threshold_value': threshold.warning_threshold if severity == AlertSeverity.WARNING else threshold.critical_threshold
            }
        )
        
        self.alert_history.append(event)
        self.alert_cooldowns[alert_name] = datetime.now() + timedelta(seconds=600)  # 10 min cooldown
        
        logger.warning(f"Alert triggered: {metric_name} = {value} ({severity.name})")
        self._emit_event(event)
    
    async def start_monitoring(self):
        """Start the monitoring system."""
        self.running = True
        logger.info("Starting monitoring system")
        
        # Start monitoring loop
        self._monitor_task = asyncio.create_task(self._monitoring_loop())
        
        # Start system metrics collection if enabled
        if self.collect_system_metrics and PSUTIL_AVAILABLE:
            asyncio.create_task(self._collect_system_metrics())
    
    async def stop_monitoring(self):
        """Stop the monitoring system."""
        self.running = False
        
        if self._monitor_task:
            await self._monitor_task
        
        logger.info("Monitoring system stopped")
    
    async def _monitoring_loop(self):
        """Main monitoring loop."""
        check_interval = self.config.get('check_interval', 10)  # seconds
        
        while self.running:
            try:
                # Check all alert rules
                await self._check_alert_rules()
                
                # Perform anomaly detection
                await self._detect_anomalies()
                
                # Sleep until next check
                await asyncio.sleep(check_interval)
                
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}")
                await asyncio.sleep(check_interval)
    
    async def _check_alert_rules(self):
        """Check all alert rules against current metrics."""
        for rule_name, rule in self.alert_rules.items():
            if not rule.enabled:
                continue
            
            # Check cooldown
            if rule_name in self.alert_cooldowns:
                if datetime.now() < self.alert_cooldowns[rule_name]:
                    continue
            
            # Get metric value
            current_value = self.metric_store.get_latest_value(rule.metric_name)
            if current_value is None:
                continue
            
            # Get historical values for advanced detection
            historical_values = self.metric_store.get_metric_values(
                rule.metric_name,
                window_seconds=rule.window_size
            )
            
            # Evaluate rule
            if rule.evaluate(current_value, historical_values):
                await self._trigger_alert(rule, current_value)
    
    async def _trigger_alert(self, rule: AlertRule, value: float):
        """Trigger an alert based on rule."""
        # Create alert event
        event = MonitoringEvent(
            timestamp=datetime.now(),
            event_type="alert_triggered",
            metric_name=rule.metric_name,
            value=value,
            severity=rule.severity,
            metadata={
                'rule_name': rule.name,
                'detection_method': rule.detection_method.value,
                'actions': rule.actions
            }
        )
        
        self.alert_history.append(event)
        self.alert_cooldowns[rule.name] = datetime.now() + timedelta(seconds=rule.cooldown_period)
        
        logger.warning(f"Alert triggered: {rule.name} - {rule.metric_name} = {value}")
        
        # Execute actions
        for action in rule.actions:
            await self._execute_alert_action(action, event)
        
        self._emit_event(event)
    
    async def _execute_alert_action(self, action: str, event: MonitoringEvent):
        """Execute an alert action."""
        logger.info(f"Executing alert action: {action}")
        
        # Action implementation would go here
        # Examples: send_email, send_slack, trigger_webhook, scale_resources
        
    async def _detect_anomalies(self):
        """Detect anomalies in metrics."""
        for metric_name, metric_def in self.metric_definitions.items():
            # Get recent values
            values = self.metric_store.get_metric_values(
                metric_name,
                window_seconds=metric_def.aggregation_window * 10  # 10 windows
            )
            
            if len(values) < 10:
                continue
            
            # Simple anomaly detection using IQR method
            q1 = statistics.quantiles(values, n=4)[0]
            q3 = statistics.quantiles(values, n=4)[2]
            iqr = q3 - q1
            
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            current_value = values[-1] if values else None
            
            if current_value and (current_value < lower_bound or current_value > upper_bound):
                event = MonitoringEvent(
                    timestamp=datetime.now(),
                    event_type="anomaly_detected",
                    metric_name=metric_name,
                    value=current_value,
                    severity=AlertSeverity.WARNING,
                    metadata={
                        'lower_bound': lower_bound,
                        'upper_bound': upper_bound,
                        'detection_method': 'iqr'
                    }
                )
                
                logger.warning(f"Anomaly detected: {metric_name} = {current_value} (bounds: {lower_bound:.2f} - {upper_bound:.2f})")
                self._emit_event(event)
    
    async def _collect_system_metrics(self):
        """Collect system-level metrics."""
        while self.running:
            try:
                # CPU usage
                cpu_percent = psutil.cpu_percent(interval=1)
                self.record_metric('system_cpu_usage', cpu_percent)
                
                # Memory usage
                memory = psutil.virtual_memory()
                self.record_metric('system_memory_usage', memory.percent)
                self.record_metric('system_memory_available_gb', memory.available / (1024**3))
                
                # Disk usage
                disk = psutil.disk_usage('/')
                self.record_metric('system_disk_usage', disk.percent)
                
                # Network I/O
                net_io = psutil.net_io_counters()
                self.record_metric('system_network_bytes_sent', net_io.bytes_sent)
                self.record_metric('system_network_bytes_recv', net_io.bytes_recv)
                
                await asyncio.sleep(30)  # Collect every 30 seconds
                
            except Exception as e:
                logger.error(f"Error collecting system metrics: {e}")
                await asyncio.sleep(30)
    
    def add_event_handler(self, handler: Callable[[MonitoringEvent], None]):
        """
        Add an event handler.
        
        Args:
            handler: Function to handle monitoring events
        """
        self.event_handlers.append(handler)
    
    def _emit_event(self, event: MonitoringEvent):
        """Emit event to all handlers."""
        for handler in self.event_handlers:
            try:
                handler(event)
            except Exception as e:
                logger.error(f"Error in event handler: {e}")
    
    def get_metrics_summary(self, window_minutes: int = 60) -> Dict[str, Any]:
        """
        Get summary of recent metrics.
        
        Args:
            window_minutes: Time window in minutes
            
        Returns:
            Summary statistics for all metrics
        """
        summary = {}
        window_seconds = window_minutes * 60
        
        for metric_name in self.metric_definitions:
            values = self.metric_store.get_metric_values(metric_name, window_seconds)
            
            if values:
                summary[metric_name] = {
                    'current': values[-1],
                    'min': min(values),
                    'max': max(values),
                    'avg': statistics.mean(values),
                    'std': statistics.stdev(values) if len(values) > 1 else 0,
                    'count': len(values)
                }
            else:
                summary[metric_name] = {
                    'current': None,
                    'min': None,
                    'max': None,
                    'avg': None,
                    'std': None,
                    'count': 0
                }
        
        return summary
    
    def get_alert_history(self, hours: int = 24) -> List[MonitoringEvent]:
        """
        Get recent alert history.
        
        Args:
            hours: Number of hours to look back
            
        Returns:
            List of recent alerts
        """
        cutoff_time = datetime.now() - timedelta(hours=hours)
        return [alert for alert in self.alert_history if alert.timestamp > cutoff_time]
    
    def export_metrics(self, format: str = "json") -> str:
        """
        Export current metrics.
        
        Args:
            format: Export format ('json', 'prometheus')
            
        Returns:
            Exported metrics string
        """
        if format == "json":
            metrics_data = {}
            
            for metric_name in self.metric_definitions:
                current_value = self.metric_store.get_latest_value(metric_name)
                recent_values = self.metric_store.get_metric_values(metric_name, window_seconds=3600)
                
                metrics_data[metric_name] = {
                    'current_value': current_value,
                    'recent_values': recent_values[-100:] if recent_values else [],  # Last 100 values
                    'timestamp': datetime.now().isoformat()
                }
            
            return json.dumps(metrics_data, indent=2)
        
        elif format == "prometheus" and PROMETHEUS_AVAILABLE:
            # Return Prometheus format metrics
            from prometheus_client import generate_latest
            return generate_latest().decode('utf-8')
        
        else:
            raise ValueError(f"Unsupported export format: {format}")
    
    def create_default_pm_monitoring(self):
        """Create default monitoring configuration for PM metrics."""
        # Define standard PM metrics
        pm_metrics = [
            MetricDefinition(
                name="pm_velocity",
                metric_type=MetricType.GAUGE,
                description="Team velocity in story points per sprint",
                unit="points",
                labels=["team", "sprint"]
            ),
            MetricDefinition(
                name="pm_burnout_risk",
                metric_type=MetricType.GAUGE,
                description="PM burnout risk score (0-100)",
                unit="score",
                labels=["pm_name"]
            ),
            MetricDefinition(
                name="pm_quality_score",
                metric_type=MetricType.GAUGE,
                description="Code quality score (0-100)",
                unit="score",
                labels=["team", "project"]
            ),
            MetricDefinition(
                name="pm_on_time_delivery",
                metric_type=MetricType.GAUGE,
                description="On-time delivery rate",
                unit="percentage",
                labels=["team"]
            ),
            MetricDefinition(
                name="pm_meeting_hours",
                metric_type=MetricType.GAUGE,
                description="Hours spent in meetings",
                unit="hours",
                labels=["pm_name"]
            ),
            MetricDefinition(
                name="pm_response_time",
                metric_type=MetricType.HISTOGRAM,
                description="PM response time to requests",
                unit="seconds",
                labels=["request_type"],
                buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
            )
        ]
        
        for metric in pm_metrics:
            self.define_metric(metric)
        
        # Define performance thresholds
        thresholds = [
            PerformanceThreshold(
                metric_name="pm_velocity",
                warning_threshold=20,
                critical_threshold=10,
                direction="below"
            ),
            PerformanceThreshold(
                metric_name="pm_burnout_risk",
                warning_threshold=60,
                critical_threshold=80,
                direction="above"
            ),
            PerformanceThreshold(
                metric_name="pm_quality_score",
                warning_threshold=70,
                critical_threshold=50,
                direction="below"
            ),
            PerformanceThreshold(
                metric_name="pm_meeting_hours",
                warning_threshold=30,
                critical_threshold=40,
                direction="above"
            )
        ]
        
        for threshold in thresholds:
            self.set_performance_threshold(threshold)
        
        # Define alert rules
        alert_rules = [
            AlertRule(
                name="velocity_drop",
                metric_name="pm_velocity",
                condition="value < 15",
                severity=AlertSeverity.WARNING,
                detection_method=AnomalyDetectionMethod.TREND,
                window_size=1800,  # 30 minutes
                actions=["notify_pm_lead", "schedule_retrospective"]
            ),
            AlertRule(
                name="high_burnout_risk",
                metric_name="pm_burnout_risk",
                condition="value > 75",
                severity=AlertSeverity.CRITICAL,
                detection_method=AnomalyDetectionMethod.THRESHOLD,
                threshold_value=75,
                window_size=3600,  # 1 hour
                actions=["notify_hr", "trigger_wellness_check"]
            ),
            AlertRule(
                name="quality_degradation",
                metric_name="pm_quality_score",
                condition="value < 60",
                severity=AlertSeverity.ERROR,
                detection_method=AnomalyDetectionMethod.STATISTICAL,
                window_size=7200,  # 2 hours
                actions=["notify_tech_lead", "increase_code_reviews"]
            )
        ]
        
        for rule in alert_rules:
            self.add_alert_rule(rule)
        
        logger.info("Created default PM monitoring configuration")
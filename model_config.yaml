# Model Configuration for Vault Query System
# Supports Ollama and custom model integration

# Available model providers
providers:
  ollama:
    base_url: "http://localhost:11434"
    timeout: 300
    
# Model definitions - Optimized for dolphin3
models:
  # Intent classification - using dolphin3 for superior understanding
  intent_classifier:
    provider: "ollama"
    model: "dolphin3:latest"
    temperature: 0.1
    max_tokens: 100
    description: "Classifies query intent (search, count, summarize, etc.)"
    
  # General Q&A - dolphin3 excels at natural conversation
  general_qa:
    provider: "ollama"
    model: "dolphin3:latest"
    temperature: 0.7
    max_tokens: 500
    description: "General question answering about vault content"
    
  # Code and technical analysis - dolphin3 handles code well
  code_analyzer:
    provider: "ollama"
    model: "dolphin3:latest"
    temperature: 0.3
    max_tokens: 1000
    description: "Analyzes code snippets and technical content"
    
  # Summarization tasks - dolphin3 excellent at summarization
  summarizer:
    provider: "ollama"
    model: "dolphin3:latest"
    temperature: 0.5
    max_tokens: 300
    description: "Summarizes long documents and aggregates information"
    
  # Entity extraction - dolphin3 can extract entities effectively
  entity_extractor:
    provider: "ollama"
    model: "dolphin3:latest"
    temperature: 0.1
    max_tokens: 200
    description: "Extracts names, dates, projects from text"
    
# Ensemble strategies
ensemble:
  strategies:
    voting:
      enabled: true
      min_agreement: 0.6
      
    weighted_confidence:
      enabled: true
      confidence_threshold: 0.7
      
    cascade:
      enabled: true
      escalation_threshold: 0.5
      
    specialized:
      enabled: true
      routing_rules:
        - pattern: "code|function|class|method"
          model: "code_analyzer"
        - pattern: "summarize|summary|overview"
          model: "summarizer"
        - pattern: "who|person|people|team"
          model: "entity_extractor"
          
# Query routing configuration
routing:
  # Use intent classifier first
  use_classifier: true
  
  # Fallback order if primary model fails
  fallback_chain:
    - "general_qa"
    - "phi:2.7b"
    
  # Cache settings
  cache:
    enabled: true
    ttl_seconds: 3600
    max_size_mb: 100
    
# Performance settings
performance:
  # Maximum concurrent model calls
  max_concurrent: 3
  
  # Timeout for model responses
  response_timeout: 60
  
  # Batch processing for multiple queries
  batch_size: 5
  
  # Resource limits
  max_memory_gb: 8
  
# Data collection for improvement
feedback:
  # Collect query-response pairs
  collect_queries: true
  
  # Path to store feedback data
  data_path: "./llm_feedback"
  
  # Minimum confidence to auto-save
  auto_save_threshold: 0.8